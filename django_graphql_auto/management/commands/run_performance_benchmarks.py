"""
Commande de gestion Django pour exÃ©cuter les benchmarks de performance.

Cette commande permet d'exÃ©cuter facilement les benchmarks de performance
depuis la ligne de commande avec diverses options de configuration.
"""

import os
import sys
from django.core.management.base import BaseCommand, CommandError
from django.conf import settings

from benchmarks.performance_benchmarks import (
    PerformanceBenchmark,
    run_full_benchmark_suite
)


class Command(BaseCommand):
    """
    Commande Django pour exÃ©cuter les benchmarks de performance.
    
    Usage:
        python manage.py run_performance_benchmarks
        python manage.py run_performance_benchmarks --test n_plus_one
        python manage.py run_performance_benchmarks --output-dir ./results
        python manage.py run_performance_benchmarks --data-sizes 10,50,100
    """
    
    help = 'ExÃ©cute les benchmarks de performance pour Django GraphQL Auto'
    
    def add_arguments(self, parser):
        """Ajoute les arguments de la commande."""
        parser.add_argument(
            '--test',
            type=str,
            choices=['n_plus_one', 'caching', 'load', 'complexity', 'all'],
            default='all',
            help='Type de test Ã  exÃ©cuter (dÃ©faut: all)'
        )
        
        parser.add_argument(
            '--output-dir',
            type=str,
            default='benchmark_results',
            help='RÃ©pertoire de sortie pour les rÃ©sultats (dÃ©faut: benchmark_results)'
        )
        
        parser.add_argument(
            '--data-sizes',
            type=str,
            default='10,50,100',
            help='Tailles de donnÃ©es pour les tests N+1 (dÃ©faut: 10,50,100)'
        )
        
        parser.add_argument(
            '--concurrent-users',
            type=str,
            default='1,5,10',
            help='Nombres d\'utilisateurs concurrents pour les tests de charge (dÃ©faut: 1,5,10)'
        )
        
        parser.add_argument(
            '--requests-per-user',
            type=int,
            default=10,
            help='Nombre de requÃªtes par utilisateur pour les tests de charge (dÃ©faut: 10)'
        )
        
        parser.add_argument(
            '--query-depths',
            type=str,
            default='1,3,5',
            help='Profondeurs de requÃªtes pour les tests de complexitÃ© (dÃ©faut: 1,3,5)'
        )
        
        parser.add_argument(
            '--cache-scenarios',
            type=str,
            default='no_cache,query_cache,field_cache,full_cache',
            help='ScÃ©narios de cache Ã  tester (dÃ©faut: no_cache,query_cache,field_cache,full_cache)'
        )
        
        parser.add_argument(
            '--verbose',
            action='store_true',
            help='Affichage verbeux des rÃ©sultats'
        )
        
        parser.add_argument(
            '--no-cleanup',
            action='store_true',
            help='Ne pas nettoyer les donnÃ©es de test aprÃ¨s les benchmarks'
        )
    
    def handle(self, *args, **options):
        """ExÃ©cute la commande."""
        self.stdout.write(
            self.style.SUCCESS('ðŸš€ DÃ©marrage des benchmarks de performance...')
        )
        
        # VÃ©rifier la configuration
        self._check_configuration()
        
        # CrÃ©er l'instance de benchmark
        benchmark = PerformanceBenchmark(output_dir=options['output_dir'])
        
        try:
            # ExÃ©cuter les tests selon les options
            if options['test'] == 'all':
                self._run_all_benchmarks(benchmark, options)
            elif options['test'] == 'n_plus_one':
                self._run_n_plus_one_benchmark(benchmark, options)
            elif options['test'] == 'caching':
                self._run_caching_benchmark(benchmark, options)
            elif options['test'] == 'load':
                self._run_load_benchmark(benchmark, options)
            elif options['test'] == 'complexity':
                self._run_complexity_benchmark(benchmark, options)
            
            # GÃ©nÃ©rer le rapport
            summary = benchmark.generate_report()
            
            # Afficher les rÃ©sultats
            self._display_results(summary, options['verbose'])
            
            self.stdout.write(
                self.style.SUCCESS('âœ… Benchmarks terminÃ©s avec succÃ¨s!')
            )
            
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ Erreur lors des benchmarks: {e}')
            )
            if options['verbose']:
                import traceback
                self.stdout.write(traceback.format_exc())
            raise CommandError(f'Ã‰chec des benchmarks: {e}')
    
    def _check_configuration(self):
        """VÃ©rifie la configuration Django."""
        # VÃ©rifier que les apps nÃ©cessaires sont installÃ©es
        required_apps = [
            'django.contrib.auth',
            'django.contrib.contenttypes',
            'django_graphql_auto'
        ]
        
        for app in required_apps:
            if app not in settings.INSTALLED_APPS:
                raise CommandError(f'App requise manquante: {app}')
        
        # VÃ©rifier la configuration de la base de donnÃ©es
        if not settings.DATABASES:
            raise CommandError('Configuration de base de donnÃ©es manquante')
        
        # VÃ©rifier la configuration du cache
        if not hasattr(settings, 'CACHES'):
            self.stdout.write(
                self.style.WARNING('âš ï¸  Configuration de cache manquante, utilisation du cache par dÃ©faut')
            )
    
    def _parse_int_list(self, value: str) -> list:
        """Parse une liste d'entiers sÃ©parÃ©s par des virgules."""
        try:
            return [int(x.strip()) for x in value.split(',')]
        except ValueError as e:
            raise CommandError(f'Format de liste invalide: {value}. Erreur: {e}')
    
    def _parse_string_list(self, value: str) -> list:
        """Parse une liste de chaÃ®nes sÃ©parÃ©es par des virgules."""
        return [x.strip() for x in value.split(',')]
    
    def _run_all_benchmarks(self, benchmark: PerformanceBenchmark, options: dict):
        """ExÃ©cute tous les benchmarks."""
        self.stdout.write('ðŸ“Š ExÃ©cution de tous les benchmarks...')
        
        # N+1 benchmark
        data_sizes = self._parse_int_list(options['data_sizes'])
        benchmark.run_n_plus_one_benchmark(data_sizes)
        
        # Caching benchmark
        cache_scenarios = self._parse_string_list(options['cache_scenarios'])
        benchmark.run_caching_benchmark(cache_scenarios)
        
        # Load test
        concurrent_users = self._parse_int_list(options['concurrent_users'])
        benchmark.run_load_test(concurrent_users, options['requests_per_user'])
        
        # Complexity benchmark
        query_depths = self._parse_int_list(options['query_depths'])
        benchmark.run_complexity_benchmark(query_depths)
    
    def _run_n_plus_one_benchmark(self, benchmark: PerformanceBenchmark, options: dict):
        """ExÃ©cute le benchmark N+1."""
        self.stdout.write('ðŸ“Š ExÃ©cution du benchmark N+1...')
        data_sizes = self._parse_int_list(options['data_sizes'])
        benchmark.run_n_plus_one_benchmark(data_sizes)
    
    def _run_caching_benchmark(self, benchmark: PerformanceBenchmark, options: dict):
        """ExÃ©cute le benchmark de cache."""
        self.stdout.write('ðŸ“Š ExÃ©cution du benchmark de cache...')
        cache_scenarios = self._parse_string_list(options['cache_scenarios'])
        benchmark.run_caching_benchmark(cache_scenarios)
    
    def _run_load_benchmark(self, benchmark: PerformanceBenchmark, options: dict):
        """ExÃ©cute le test de charge."""
        self.stdout.write('ðŸ“Š ExÃ©cution du test de charge...')
        concurrent_users = self._parse_int_list(options['concurrent_users'])
        benchmark.run_load_test(concurrent_users, options['requests_per_user'])
    
    def _run_complexity_benchmark(self, benchmark: PerformanceBenchmark, options: dict):
        """ExÃ©cute le benchmark de complexitÃ©."""
        self.stdout.write('ðŸ“Š ExÃ©cution du benchmark de complexitÃ©...')
        query_depths = self._parse_int_list(options['query_depths'])
        benchmark.run_complexity_benchmark(query_depths)
    
    def _display_results(self, summary, verbose: bool):
        """Affiche les rÃ©sultats des benchmarks."""
        self.stdout.write('\n' + '='*60)
        self.stdout.write(self.style.SUCCESS('ðŸ“Š RÃ‰SULTATS DES BENCHMARKS'))
        self.stdout.write('='*60)
        
        # RÃ©sumÃ© principal
        self.stdout.write(f'Tests total: {summary.total_tests}')
        self.stdout.write(
            self.style.SUCCESS(f'SuccÃ¨s: {summary.successful_tests}')
        )
        if summary.failed_tests > 0:
            self.stdout.write(
                self.style.ERROR(f'Ã‰checs: {summary.failed_tests}')
            )
        
        self.stdout.write(f'Temps d\'exÃ©cution moyen: {summary.avg_execution_time:.3f}s')
        self.stdout.write(f'Temps minimum: {summary.min_execution_time:.3f}s')
        self.stdout.write(f'Temps maximum: {summary.max_execution_time:.3f}s')
        
        # MÃ©triques de performance
        self.stdout.write('\nðŸ“ˆ MÃ‰TRIQUES DE PERFORMANCE:')
        self.stdout.write(f'RequÃªtes de base de donnÃ©es: {summary.total_database_queries}')
        self.stdout.write(f'Cache hits: {summary.total_cache_hits}')
        self.stdout.write(f'Cache misses: {summary.total_cache_misses}')
        
        # AmÃ©lioration des performances
        if summary.performance_improvement > 0:
            self.stdout.write(
                self.style.SUCCESS(
                    f'\nðŸš€ AMÃ‰LIORATION DES PERFORMANCES: {summary.performance_improvement:.1f}%'
                )
            )
        elif summary.performance_improvement < 0:
            self.stdout.write(
                self.style.WARNING(
                    f'\nâš ï¸  DÃ‰GRADATION DES PERFORMANCES: {abs(summary.performance_improvement):.1f}%'
                )
            )
        else:
            self.stdout.write('\nâž¡ï¸  Aucune amÃ©lioration mesurable des performances')
        
        # Recommandations
        self._display_recommendations(summary)
        
        # Informations sur les fichiers de sortie
        self.stdout.write(f'\nðŸ“ RÃ©sultats sauvegardÃ©s dans: benchmark_results/')
        self.stdout.write('   - benchmark_results_*.json (donnÃ©es dÃ©taillÃ©es)')
        self.stdout.write('   - benchmark_results_*.csv (format tableur)')
        self.stdout.write('   - benchmark_report_*.html (rapport visuel)')
    
    def _display_recommendations(self, summary):
        """Affiche des recommandations basÃ©es sur les rÃ©sultats."""
        self.stdout.write('\nðŸ’¡ RECOMMANDATIONS:')
        
        # Recommandations basÃ©es sur les performances
        if summary.performance_improvement < 10:
            self.stdout.write('â€¢ ConsidÃ©rez l\'activation de plus d\'optimisations')
            self.stdout.write('â€¢ VÃ©rifiez la configuration du cache')
        
        if summary.total_database_queries > summary.successful_tests * 2:
            self.stdout.write('â€¢ Nombre Ã©levÃ© de requÃªtes DB dÃ©tectÃ©')
            self.stdout.write('â€¢ VÃ©rifiez l\'implÃ©mentation de select_related/prefetch_related')
        
        cache_hit_ratio = 0
        if summary.total_cache_hits + summary.total_cache_misses > 0:
            cache_hit_ratio = summary.total_cache_hits / (summary.total_cache_hits + summary.total_cache_misses)
        
        if cache_hit_ratio < 0.5:
            self.stdout.write('â€¢ Taux de cache hit faible dÃ©tectÃ©')
            self.stdout.write('â€¢ ConsidÃ©rez l\'ajustement de la stratÃ©gie de cache')
        
        if summary.avg_execution_time > 1.0:
            self.stdout.write('â€¢ Temps d\'exÃ©cution Ã©levÃ© dÃ©tectÃ©')
            self.stdout.write('â€¢ ConsidÃ©rez l\'optimisation des requÃªtes complexes')
        
        if summary.failed_tests > 0:
            self.stdout.write(self.style.WARNING('â€¢ Des tests ont Ã©chouÃ©, vÃ©rifiez les logs'))
    
    def _cleanup_on_exit(self):
        """Nettoie les ressources Ã  la sortie."""
        # Cette mÃ©thode peut Ãªtre Ã©tendue pour nettoyer les donnÃ©es de test
        pass