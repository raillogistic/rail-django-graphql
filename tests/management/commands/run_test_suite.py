"""
Commande de gestion Django pour exÃ©cuter la suite de tests complÃ¨te.

Cette commande fournit:
- ExÃ©cution de tests avec configuration personnalisÃ©e
- Rapports dÃ©taillÃ©s de couverture
- MÃ©triques de performance
- Nettoyage automatique
"""

import os
import sys
import time
from datetime import datetime
from django.core.management.base import BaseCommand, CommandError
from django.conf import settings
from django.test.utils import get_runner
from django.db import connection


class Command(BaseCommand):
    """Commande pour exÃ©cuter la suite de tests complÃ¨te."""
    
    help = 'ExÃ©cute la suite de tests complÃ¨te avec rapports dÃ©taillÃ©s'
    
    def add_arguments(self, parser):
        """Ajoute les arguments de la commande."""
        parser.add_argument(
            '--pattern',
            type=str,
            default='test_*.py',
            help='Motif des fichiers de test Ã  exÃ©cuter'
        )
        
        parser.add_argument(
            '--coverage',
            action='store_true',
            help='Active le rapport de couverture'
        )
        
        parser.add_argument(
            '--performance',
            action='store_true',
            help='Active les tests de performance'
        )
        
        parser.add_argument(
            '--parallel',
            type=int,
            default=1,
            help='Nombre de processus parallÃ¨les'
        )
        
        parser.add_argument(
            '--failfast',
            action='store_true',
            help='ArrÃªte Ã  la premiÃ¨re erreur'
        )
        
        parser.add_argument(
            '--keepdb',
            action='store_true',
            help='Conserve la base de donnÃ©es de test'
        )
        
        parser.add_argument(
            '--debug-mode',
            action='store_true',
            help='Active le mode debug'
        )
        
        parser.add_argument(
            '--output-dir',
            type=str,
            default='test_reports',
            help='RÃ©pertoire de sortie des rapports'
        )
        
        parser.add_argument(
            '--exclude-tags',
            type=str,
            nargs='*',
            default=[],
            help='Tags de tests Ã  exclure'
        )
        
        parser.add_argument(
            '--include-tags',
            type=str,
            nargs='*',
            default=[],
            help='Tags de tests Ã  inclure uniquement'
        )
    
    def handle(self, *args, **options):
        """ExÃ©cute la commande."""
        start_time = time.time()
        
        self.stdout.write(
            self.style.SUCCESS('ğŸš€ DÃ©marrage de la suite de tests complÃ¨te')
        )
        
        # Configuration de l'environnement de test
        self._setup_test_environment(options)
        
        # PrÃ©paration des rapports
        self._prepare_reports_directory(options['output_dir'])
        
        # ExÃ©cution des tests
        try:
            results = self._run_tests(options)
            
            # GÃ©nÃ©ration des rapports
            self._generate_reports(results, options)
            
            # Affichage du rÃ©sumÃ©
            self._display_summary(results, start_time)
            
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ Erreur lors de l\'exÃ©cution des tests: {e}')
            )
            raise CommandError(f'Ã‰chec de l\'exÃ©cution des tests: {e}')
        
        finally:
            # Nettoyage
            if not options['keepdb']:
                self._cleanup_test_environment()
    
    def _setup_test_environment(self, options):
        """Configure l'environnement de test."""
        self.stdout.write('âš™ï¸  Configuration de l\'environnement de test...')
        
        # Variables d'environnement
        os.environ['TESTING'] = 'True'
        os.environ['DJANGO_SETTINGS_MODULE'] = 'tests.settings'
        
        # Configuration du debug
        if options['debug_mode']:
            os.environ['DEBUG'] = 'True'
        
        # Configuration de la couverture
        if options['coverage']:
            try:
                import coverage
                self.coverage = coverage.Coverage(
                    source=['rail_django_graphql'],
                    omit=[
                        '*/tests/*',
                        '*/migrations/*',
                        '*/venv/*',
                        '*/env/*',
                    ]
                )
                self.coverage.start()
                self.stdout.write('âœ… Couverture de code activÃ©e')
            except ImportError:
                self.stdout.write(
                    self.style.WARNING('âš ï¸  Module coverage non disponible')
                )
                options['coverage'] = False
    
    def _prepare_reports_directory(self, output_dir):
        """PrÃ©pare le rÃ©pertoire des rapports."""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            self.stdout.write(f'ğŸ“ RÃ©pertoire de rapports crÃ©Ã©: {output_dir}')
    
    def _run_tests(self, options):
        """ExÃ©cute les tests."""
        self.stdout.write('ğŸ§ª ExÃ©cution des tests...')
        
        # Configuration du runner de tests
        test_runner_class = get_runner(settings)
        test_runner = test_runner_class(
            verbosity=2,
            interactive=False,
            failfast=options['failfast'],
            keepdb=options['keepdb'],
            parallel=options['parallel'] if options['parallel'] > 1 else 0,
        )
        
        # Labels de tests Ã  exÃ©cuter
        test_labels = self._get_test_labels(options)
        
        # ExÃ©cution
        start_time = time.time()
        
        try:
            result = test_runner.run_tests(test_labels)
            execution_time = time.time() - start_time
            
            return {
                'result': result,
                'execution_time': execution_time,
                'test_count': getattr(test_runner, 'test_count', 0),
                'failure_count': getattr(test_runner, 'failure_count', 0),
                'error_count': getattr(test_runner, 'error_count', 0),
            }
        
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ Erreur pendant l\'exÃ©cution: {e}')
            )
            raise
    
    def _get_test_labels(self, options):
        """DÃ©termine les labels de tests Ã  exÃ©cuter."""
        labels = []
        
        # Tests de base
        labels.extend([
            'tests.test_core',
            'tests.test_generators',
            'tests.test_integration',
            'tests.test_business_methods',
        ])
        
        # Tests de performance si demandÃ©s
        if options['performance']:
            labels.extend([
                'tests.test_performance',
            ])
        
        # Filtrage par tags
        if options['include_tags']:
            # Logique pour inclure seulement certains tags
            pass
        
        if options['exclude_tags']:
            # Logique pour exclure certains tags
            pass
        
        return labels
    
    def _generate_reports(self, results, options):
        """GÃ©nÃ¨re les rapports de test."""
        self.stdout.write('ğŸ“Š GÃ©nÃ©ration des rapports...')
        
        output_dir = options['output_dir']
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Rapport de couverture
        if options['coverage'] and hasattr(self, 'coverage'):
            self.coverage.stop()
            
            # Rapport HTML
            html_dir = os.path.join(output_dir, f'coverage_html_{timestamp}')
            self.coverage.html_report(directory=html_dir)
            
            # Rapport XML
            xml_file = os.path.join(output_dir, f'coverage_{timestamp}.xml')
            self.coverage.xml_report(outfile=xml_file)
            
            self.stdout.write(f'âœ… Rapport de couverture gÃ©nÃ©rÃ©: {html_dir}')
        
        # Rapport de performance
        if options['performance']:
            self._generate_performance_report(results, output_dir, timestamp)
        
        # Rapport de rÃ©sumÃ©
        self._generate_summary_report(results, output_dir, timestamp)
    
    def _generate_performance_report(self, results, output_dir, timestamp):
        """GÃ©nÃ¨re le rapport de performance."""
        try:
            from tests.models import TestPerformanceModel
            
            perf_data = TestPerformanceModel.objects.all().order_by('-created_at')[:100]
            
            report_file = os.path.join(output_dir, f'performance_{timestamp}.json')
            
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'total_tests': len(perf_data),
                    'tests': [
                        {
                            'name': test.name,
                            'execution_time': test.execution_time,
                            'memory_usage': test.memory_usage,
                            'created_at': test.created_at.isoformat(),
                        }
                        for test in perf_data
                    ]
                }, f, indent=2, ensure_ascii=False)
            
            self.stdout.write(f'âœ… Rapport de performance gÃ©nÃ©rÃ©: {report_file}')
        
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f'âš ï¸  Erreur gÃ©nÃ©ration rapport performance: {e}')
            )
    
    def _generate_summary_report(self, results, output_dir, timestamp):
        """GÃ©nÃ¨re le rapport de rÃ©sumÃ©."""
        report_file = os.path.join(output_dir, f'summary_{timestamp}.json')
        
        summary = {
            'timestamp': timestamp,
            'execution_time': results['execution_time'],
            'test_result': results['result'],
            'test_count': results.get('test_count', 0),
            'failure_count': results.get('failure_count', 0),
            'error_count': results.get('error_count', 0),
            'success_rate': self._calculate_success_rate(results),
            'database_queries': len(connection.queries),
            'environment': {
                'python_version': sys.version,
                'django_version': self._get_django_version(),
                'testing_mode': os.environ.get('TESTING', 'False'),
            }
        }
        
        import json
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        self.stdout.write(f'âœ… Rapport de rÃ©sumÃ© gÃ©nÃ©rÃ©: {report_file}')
    
    def _calculate_success_rate(self, results):
        """Calcule le taux de succÃ¨s."""
        total = results.get('test_count', 0)
        failures = results.get('failure_count', 0)
        errors = results.get('error_count', 0)
        
        if total == 0:
            return 0.0
        
        success = total - failures - errors
        return (success / total) * 100
    
    def _get_django_version(self):
        """RÃ©cupÃ¨re la version de Django."""
        try:
            import django
            return django.get_version()
        except Exception:
            return 'Unknown'
    
    def _display_summary(self, results, start_time):
        """Affiche le rÃ©sumÃ© des rÃ©sultats."""
        total_time = time.time() - start_time
        
        self.stdout.write('\n' + '='*60)
        self.stdout.write(self.style.SUCCESS('ğŸ“‹ RÃ‰SUMÃ‰ DE L\'EXÃ‰CUTION DES TESTS'))
        self.stdout.write('='*60)
        
        # RÃ©sultats gÃ©nÃ©raux
        self.stdout.write(f'â±ï¸  Temps total: {total_time:.2f} secondes')
        self.stdout.write(f'ğŸ§ª Tests exÃ©cutÃ©s: {results.get("test_count", 0)}')
        self.stdout.write(f'âŒ Ã‰checs: {results.get("failure_count", 0)}')
        self.stdout.write(f'ğŸ’¥ Erreurs: {results.get("error_count", 0)}')
        
        # Taux de succÃ¨s
        success_rate = self._calculate_success_rate(results)
        if success_rate >= 95:
            style = self.style.SUCCESS
            icon = 'âœ…'
        elif success_rate >= 80:
            style = self.style.WARNING
            icon = 'âš ï¸'
        else:
            style = self.style.ERROR
            icon = 'âŒ'
        
        self.stdout.write(f'{icon} Taux de succÃ¨s: {style(f"{success_rate:.1f}%")}')
        
        # RequÃªtes base de donnÃ©es
        self.stdout.write(f'ğŸ—„ï¸  RequÃªtes DB: {len(connection.queries)}')
        
        # Statut final
        if results['result'] == 0:
            self.stdout.write(self.style.SUCCESS('\nğŸ‰ TOUS LES TESTS ONT RÃ‰USSI!'))
        else:
            self.stdout.write(self.style.ERROR('\nğŸ’¥ CERTAINS TESTS ONT Ã‰CHOUÃ‰!'))
        
        self.stdout.write('='*60)
    
    def _cleanup_test_environment(self):
        """Nettoie l'environnement de test."""
        self.stdout.write('ğŸ§¹ Nettoyage de l\'environnement de test...')
        
        # ArrÃªter la couverture si active
        if hasattr(self, 'coverage'):
            try:
                self.coverage.stop()
            except Exception:
                pass
        
        # Nettoyer les variables d'environnement
        env_vars_to_clean = ['TESTING', 'DEBUG']
        for var in env_vars_to_clean:
            if var in os.environ:
                del os.environ[var]
        
        self.stdout.write('âœ… Nettoyage terminÃ©')
