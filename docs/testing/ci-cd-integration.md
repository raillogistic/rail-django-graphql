# Int√©gration CI/CD - Tests Django GraphQL Auto

Ce guide d√©taille l'int√©gration des tests dans les pipelines CI/CD pour assurer la qualit√© et la fiabilit√© du projet Django GraphQL Auto.

## üìã Table des Mati√®res

- [Vue d'Ensemble](#vue-densemble)
- [GitHub Actions](#github-actions)
- [Configuration des Workflows](#configuration-des-workflows)
- [Tests Automatis√©s](#tests-automatis√©s)
- [D√©ploiement Continu](#d√©ploiement-continu)
- [Monitoring et Alertes](#monitoring-et-alertes)
- [Optimisation des Performances](#optimisation-des-performances)
- [S√©curit√©](#s√©curit√©)

## üéØ Vue d'Ensemble

### Strat√©gie CI/CD

```yaml
# Flux de d√©veloppement
D√©veloppement ‚Üí Tests Locaux ‚Üí Pull Request ‚Üí Tests CI ‚Üí Review ‚Üí Merge ‚Üí D√©ploiement

# Environnements
- development: Tests complets + d√©ploiement automatique
- staging: Tests de r√©gression + validation manuelle
- production: Tests de fum√©e + d√©ploiement contr√¥l√©
```

### Objectifs de Qualit√©

```yaml
quality_gates:
  test_coverage: ">= 80%"
  performance_threshold: "< 2s pour les tests unitaires"
  security_scan: "Aucune vuln√©rabilit√© critique"
  code_quality: "Grade A (SonarQube)"
  documentation: "100% des API publiques document√©es"
```

## üîÑ GitHub Actions

### 1. Workflow Principal

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Tests nocturnes √† 2h00 UTC
    - cron: "0 2 * * *"

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
  DJANGO_SETTINGS_MODULE: "tests.settings"
  TESTING: "true"

jobs:
  # ===== TESTS DE BASE =====
  test:
    name: Tests (${{ matrix.python-version }}, ${{ matrix.django-version }})
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
        django-version: ["4.2", "5.0", "5.1"]
        exclude:
          # Django 5.0+ n√©cessite Python 3.10+
          - python-version: "3.9"
            django-version: "5.0"
          - python-version: "3.9"
            django-version: "5.1"

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # N√©cessaire pour SonarQube

      - name: Configuration Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Installation des d√©pendances
        run: |
          python -m pip install --upgrade pip
          pip install Django==${{ matrix.django-version }}
          pip install -r requirements-test.txt
          pip install -e .

      - name: V√©rification du code (linting)
        run: |
          flake8 rail_django_graphql tests
          black --check rail_django_graphql tests
          isort --check-only rail_django_graphql tests
          mypy rail_django_graphql

      - name: Tests de s√©curit√©
        run: |
          bandit -r rail_django_graphql
          safety check

      - name: Tests unitaires et d'int√©gration
        run: |
          pytest \
            --cov=rail_django_graphql \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            --junitxml=test-results.xml \
            --durations=10 \
            -v
        env:
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0

      - name: Tests de performance
        run: |
          pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=benchmark.json

      - name: Upload des r√©sultats de couverture
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: Upload des artefacts de test
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}-${{ matrix.django-version }}
          path: |
            test-results.xml
            htmlcov/
            benchmark.json
          retention-days: 30

  # ===== ANALYSE DE QUALIT√â =====
  quality:
    name: Analyse de qualit√©
    runs-on: ubuntu-latest
    needs: test

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configuration Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Installation des d√©pendances
        run: |
          pip install -r requirements-test.txt
          pip install -e .

      - name: G√©n√©ration du rapport de couverture
        run: |
          pytest --cov=rail_django_graphql --cov-report=xml

      - name: Analyse SonarQube
        uses: sonarqube-quality-gate-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          scanMetadataReportFile: .scannerwork/report-task.txt

      - name: Analyse de complexit√©
        run: |
          radon cc rail_django_graphql --json > complexity.json
          radon mi rail_django_graphql --json > maintainability.json

      - name: Upload des m√©triques
        uses: actions/upload-artifact@v3
        with:
          name: quality-metrics
          path: |
            complexity.json
            maintainability.json
            coverage.xml

  # ===== TESTS E2E =====
  e2e:
    name: Tests End-to-End
    runs-on: ubuntu-latest
    needs: test

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Configuration Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Configuration Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"

      - name: Installation des d√©pendances
        run: |
          pip install -r requirements-test.txt
          pip install -e .
          npm install

      - name: D√©marrage du serveur de test
        run: |
          python manage.py runserver 8000 &
          sleep 10  # Attendre que le serveur d√©marre
        env:
          DJANGO_SETTINGS_MODULE: "tests.settings"

      - name: Tests E2E avec Playwright
        run: |
          npx playwright test

      - name: Upload des rapports E2E
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-results
          path: |
            test-results/
            playwright-report/

  # ===== TESTS DE S√âCURIT√â =====
  security:
    name: Tests de s√©curit√©
    runs-on: ubuntu-latest

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Scan de s√©curit√© avec Trivy
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload des r√©sultats vers GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: "trivy-results.sarif"

      - name: Scan des d√©pendances
        run: |
          pip install safety
          safety check --json --output safety-report.json

      - name: Tests de p√©n√©tration OWASP ZAP
        uses: zaproxy/action-baseline@v0.7.0
        with:
          target: "http://localhost:8000"
          rules_file_name: ".zap/rules.tsv"

  # ===== BUILD ET PACKAGING =====
  build:
    name: Build et packaging
    runs-on: ubuntu-latest
    needs: [test, quality, security]

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Configuration Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Installation des outils de build
        run: |
          pip install build twine

      - name: Build du package
        run: |
          python -m build

      - name: V√©rification du package
        run: |
          twine check dist/*

      - name: Upload des artefacts
        uses: actions/upload-artifact@v3
        with:
          name: dist
          path: dist/

  # ===== D√âPLOIEMENT =====
  deploy:
    name: D√©ploiement
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    environment:
      name: production
      url: https://django-graphql-auto.example.com

    steps:
      - name: T√©l√©chargement des artefacts
        uses: actions/download-artifact@v3
        with:
          name: dist
          path: dist/

      - name: Publication sur PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_API_TOKEN }}
          skip_existing: true

      - name: Cr√©ation du tag de release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ github.run_number }}
          release_name: Release v${{ github.run_number }}
          draft: false
          prerelease: false
```

### 2. Workflow de Tests Nocturnes

```yaml
# .github/workflows/nightly.yml
name: Tests Nocturnes

on:
  schedule:
    - cron: "0 2 * * *" # 2h00 UTC chaque jour
  workflow_dispatch: # D√©clenchement manuel

jobs:
  comprehensive-tests:
    name: Tests complets nocturnes
    runs-on: ubuntu-latest

    strategy:
      matrix:
        test-suite:
          - unit
          - integration
          - performance
          - security
          - compatibility

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Configuration de l'environnement
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Installation des d√©pendances
        run: |
          pip install -r requirements-test.txt
          pip install -e .

      - name: Ex√©cution des tests ${{ matrix.test-suite }}
        run: |
          case "${{ matrix.test-suite }}" in
            "unit")
              pytest tests/unit/ -v --durations=0
              ;;
            "integration")
              pytest tests/integration/ -v --durations=0
              ;;
            "performance")
              pytest tests/performance/ --benchmark-only --benchmark-sort=mean
              ;;
            "security")
              pytest tests/security/ -v
              bandit -r rail_django_graphql
              ;;
            "compatibility")
              pytest tests/compatibility/ -v
              ;;
          esac

      - name: G√©n√©ration du rapport
        run: |
          echo "## Rapport de Tests ${{ matrix.test-suite }}" > report.md
          echo "Date: $(date)" >> report.md
          echo "Commit: ${{ github.sha }}" >> report.md
          # Ajouter les r√©sultats des tests

      - name: Notification Slack en cas d'√©chec
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: "#dev-alerts"
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### 3. Workflow de Performance

```yaml
# .github/workflows/performance.yml
name: Tests de Performance

on:
  pull_request:
    paths:
      - "rail_django_graphql/**"
      - "tests/performance/**"
  schedule:
    - cron: "0 4 * * 1" # Lundi 4h00 UTC

jobs:
  performance-baseline:
    name: Baseline de performance
    runs-on: ubuntu-latest

    steps:
      - name: Checkout du code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2 # Pour comparer avec le commit pr√©c√©dent

      - name: Configuration Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Installation des d√©pendances
        run: |
          pip install -r requirements-test.txt
          pip install -e .

      - name: Tests de performance (commit actuel)
        run: |
          pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=current-benchmark.json \
            --benchmark-sort=mean

      - name: Checkout du commit pr√©c√©dent
        run: |
          git checkout HEAD~1
          pip install -e .

      - name: Tests de performance (commit pr√©c√©dent)
        run: |
          pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=previous-benchmark.json \
            --benchmark-sort=mean

      - name: Comparaison des performances
        run: |
          python scripts/compare_benchmarks.py \
            previous-benchmark.json \
            current-benchmark.json \
            --threshold=10  # 10% de r√©gression acceptable

      - name: Commentaire PR avec r√©sultats
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const comparison = fs.readFileSync('benchmark-comparison.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üìä Rapport de Performance\n\n${comparison}`
            });
```

## üîß Configuration des Workflows

### 1. Variables d'Environnement

```yaml
# .github/workflows/env.yml
env:
  # Configuration Python
  PYTHON_VERSION: "3.11"
  PIP_CACHE_DIR: ~/.cache/pip

  # Configuration Django
  DJANGO_SETTINGS_MODULE: "tests.settings"
  SECRET_KEY: "test-secret-key-not-for-production"

  # Configuration de test
  TESTING: "true"
  COVERAGE_THRESHOLD: "80"

  # Configuration de base de donn√©es
  DATABASE_URL: "sqlite:///:memory:"

  # Configuration de cache
  CACHE_URL: "locmem://"

  # Configuration de logging
  LOG_LEVEL: "INFO"

  # Configuration de performance
  BENCHMARK_THRESHOLD: "10" # % de r√©gression acceptable
```

### 2. Secrets GitHub

```yaml
# Secrets requis dans GitHub Settings > Secrets
secrets:
  # PyPI
  PYPI_API_TOKEN: "pypi-token-for-publishing"

  # SonarQube
  SONAR_TOKEN: "sonar-analysis-token"
  SONAR_HOST_URL: "https://sonarcloud.io"

  # Notifications
  SLACK_WEBHOOK: "https://hooks.slack.com/services/..."

  # S√©curit√©
  SNYK_TOKEN: "snyk-security-token"

  # D√©ploiement
  DEPLOY_KEY: "ssh-private-key-for-deployment"
```

### 3. Configuration des Caches

```yaml
# Configuration du cache pour optimiser les builds
cache_strategy:
  pip_cache:
    key: pip-${{ runner.os }}-${{ hashFiles('requirements*.txt') }}
    paths:
      - ~/.cache/pip

  node_cache:
    key: node-${{ runner.os }}-${{ hashFiles('package-lock.json') }}
    paths:
      - ~/.npm

  pytest_cache:
    key: pytest-${{ runner.os }}-${{ hashFiles('pytest.ini') }}
    paths:
      - .pytest_cache
```

## üìä Tests Automatis√©s

### 1. Configuration des Tests par Environnement

```python
# tests/ci_settings.py
"""Configuration sp√©cifique pour les tests CI."""

from .settings import *

# Configuration optimis√©e pour CI
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'test_db',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': 'localhost',
        'PORT': '5432',
        'OPTIONS': {
            'MAX_CONNS': 20,
        }
    }
}

# Cache Redis pour les tests d'int√©gration
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.redis.RedisCache',
        'LOCATION': 'redis://localhost:6379/0',
        'OPTIONS': {
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 20,
            }
        }
    }
}

# Logging optimis√© pour CI
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'verbose',
        },
        'file': {
            'class': 'logging.FileHandler',
            'filename': 'test.log',
            'formatter': 'verbose',
        },
    },
    'formatters': {
        'verbose': {
            'format': '[{levelname}] {asctime} {name}: {message}',
            'style': '{',
        },
    },
    'root': {
        'handlers': ['console', 'file'],
        'level': 'INFO',
    },
}

# Configuration sp√©cifique CI
CI_SPECIFIC_SETTINGS = {
    'PARALLEL_TESTING': True,
    'TEST_RUNNER': 'django.test.runner.DiscoverRunner',
    'TEST_PROCESSES': 4,
}
```

### 2. Scripts de Test Personnalis√©s

```python
# scripts/run_ci_tests.py
"""Script pour ex√©cuter les tests en environnement CI."""

import os
import sys
import subprocess
import json
from pathlib import Path


class CITestRunner:
    """Gestionnaire des tests pour l'environnement CI."""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_results = {}

    def setup_environment(self):
        """Configure l'environnement de test."""

        env_vars = {
            'DJANGO_SETTINGS_MODULE': 'tests.ci_settings',
            'TESTING': 'true',
            'PYTHONPATH': str(self.project_root),
        }

        for key, value in env_vars.items():
            os.environ[key] = value

        print(f"‚úÖ Environnement configur√©: {env_vars}")

    def run_test_suite(self, suite_name, command):
        """Ex√©cute une suite de tests."""

        print(f"\nüß™ Ex√©cution de la suite: {suite_name}")
        print(f"Commande: {command}")

        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                cwd=self.project_root
            )

            self.test_results[suite_name] = {
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'returncode': result.returncode,
            }

            if result.returncode == 0:
                print(f"‚úÖ {suite_name}: SUCC√àS")
            else:
                print(f"‚ùå {suite_name}: √âCHEC")
                print(f"Erreur: {result.stderr}")

            return result.returncode == 0

        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution de {suite_name}: {e}")
            self.test_results[suite_name] = {
                'success': False,
                'error': str(e),
            }
            return False

    def run_all_tests(self):
        """Ex√©cute toutes les suites de tests."""

        test_suites = {
            'linting': 'flake8 rail_django_graphql tests',
            'formatting': 'black --check rail_django_graphql tests',
            'imports': 'isort --check-only rail_django_graphql tests',
            'typing': 'mypy rail_django_graphql',
            'security': 'bandit -r rail_django_graphql',
            'unit_tests': 'pytest tests/unit/ -v --tb=short',
            'integration_tests': 'pytest tests/integration/ -v --tb=short',
            'performance_tests': 'pytest tests/performance/ --benchmark-only',
        }

        all_passed = True

        for suite_name, command in test_suites.items():
            success = self.run_test_suite(suite_name, command)
            if not success:
                all_passed = False

        return all_passed

    def generate_report(self):
        """G√©n√®re un rapport de test."""

        report = {
            'timestamp': datetime.now().isoformat(),
            'environment': 'CI',
            'results': self.test_results,
            'summary': {
                'total_suites': len(self.test_results),
                'passed': sum(1 for r in self.test_results.values() if r.get('success')),
                'failed': sum(1 for r in self.test_results.values() if not r.get('success')),
            }
        }

        # Sauvegarder le rapport
        report_file = self.project_root / 'test_reports' / 'ci_report.json'
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"\nüìä Rapport g√©n√©r√©: {report_file}")

        # Afficher le r√©sum√©
        summary = report['summary']
        print(f"\nüìà R√âSUM√â:")
        print(f"  Total: {summary['total_suites']}")
        print(f"  Succ√®s: {summary['passed']}")
        print(f"  √âchecs: {summary['failed']}")

        return report


def main():
    """Point d'entr√©e principal."""

    runner = CITestRunner()

    # Configuration
    runner.setup_environment()

    # Ex√©cution des tests
    all_passed = runner.run_all_tests()

    # G√©n√©ration du rapport
    report = runner.generate_report()

    # Code de sortie
    sys.exit(0 if all_passed else 1)


if __name__ == '__main__':
    main()
```

## üöÄ D√©ploiement Continu

### 1. Strat√©gie de D√©ploiement

```yaml
# Configuration de d√©ploiement par environnement
deployment_strategy:
  development:
    trigger: "push to develop branch"
    tests: "unit + integration"
    deployment: "automatic"
    rollback: "automatic on failure"

  staging:
    trigger: "push to main branch"
    tests: "full test suite"
    deployment: "automatic"
    approval: "optional"
    rollback: "manual"

  production:
    trigger: "manual or scheduled"
    tests: "full test suite + security scan"
    deployment: "manual approval required"
    rollback: "manual with confirmation"
```

### 2. Scripts de D√©ploiement

```bash
#!/bin/bash
# scripts/deploy.sh

set -e  # Arr√™ter en cas d'erreur

ENVIRONMENT=${1:-staging}
VERSION=${2:-latest}

echo "üöÄ D√©ploiement vers $ENVIRONMENT (version: $VERSION)"

# V√©rifications pr√©-d√©ploiement
echo "üîç V√©rifications pr√©-d√©ploiement..."

# V√©rifier que les tests passent
if [ "$ENVIRONMENT" = "production" ]; then
    echo "Ex√©cution des tests complets..."
    python scripts/run_ci_tests.py

    if [ $? -ne 0 ]; then
        echo "‚ùå Les tests ont √©chou√©. D√©ploiement annul√©."
        exit 1
    fi
fi

# V√©rifier la connectivit√©
echo "V√©rification de la connectivit√©..."
curl -f https://api.$ENVIRONMENT.example.com/health || {
    echo "‚ùå Impossible de joindre l'environnement $ENVIRONMENT"
    exit 1
}

# Sauvegarde pr√©-d√©ploiement
echo "üíæ Sauvegarde pr√©-d√©ploiement..."
./scripts/backup.sh $ENVIRONMENT

# D√©ploiement
echo "üì¶ D√©ploiement en cours..."

case $ENVIRONMENT in
    "development")
        # D√©ploiement simple pour dev
        rsync -av --delete ./ dev-server:/app/
        ssh dev-server "cd /app && pip install -e . && systemctl restart django-app"
        ;;

    "staging")
        # D√©ploiement avec validation pour staging
        docker build -t django-graphql-auto:$VERSION .
        docker push registry.example.com/django-graphql-auto:$VERSION
        kubectl set image deployment/django-app app=registry.example.com/django-graphql-auto:$VERSION -n staging
        kubectl rollout status deployment/django-app -n staging
        ;;

    "production")
        # D√©ploiement blue-green pour production
        echo "üîµ D√©ploiement Blue-Green..."

        # D√©ployer sur l'environnement inactif
        CURRENT_ENV=$(kubectl get service django-app-service -o jsonpath='{.spec.selector.version}')
        NEW_ENV=$([ "$CURRENT_ENV" = "blue" ] && echo "green" || echo "blue")

        echo "Environnement actuel: $CURRENT_ENV"
        echo "Nouvel environnement: $NEW_ENV"

        # D√©ployer la nouvelle version
        kubectl set image deployment/django-app-$NEW_ENV app=registry.example.com/django-graphql-auto:$VERSION
        kubectl rollout status deployment/django-app-$NEW_ENV

        # Tests de fum√©e
        echo "üí® Tests de fum√©e..."
        ./scripts/smoke_tests.sh $NEW_ENV

        if [ $? -eq 0 ]; then
            # Basculer le trafic
            echo "üîÑ Basculement du trafic..."
            kubectl patch service django-app-service -p '{"spec":{"selector":{"version":"'$NEW_ENV'"}}}'

            echo "‚úÖ D√©ploiement r√©ussi sur $NEW_ENV"

            # Attendre avant de supprimer l'ancien environnement
            echo "‚è≥ Attente de 5 minutes avant nettoyage..."
            sleep 300

            # Arr√™ter l'ancien environnement
            kubectl scale deployment django-app-$CURRENT_ENV --replicas=0

        else
            echo "‚ùå Tests de fum√©e √©chou√©s. Rollback..."
            exit 1
        fi
        ;;
esac

# Tests post-d√©ploiement
echo "üß™ Tests post-d√©ploiement..."
./scripts/post_deploy_tests.sh $ENVIRONMENT

if [ $? -eq 0 ]; then
    echo "‚úÖ D√©ploiement termin√© avec succ√®s!"

    # Notification
    curl -X POST -H 'Content-type: application/json' \
        --data '{"text":"‚úÖ D√©ploiement r√©ussi sur '$ENVIRONMENT' (version: '$VERSION')"}' \
        $SLACK_WEBHOOK
else
    echo "‚ùå Tests post-d√©ploiement √©chou√©s!"
    exit 1
fi
```

## üìà Monitoring et Alertes

### 1. Configuration des Alertes

```yaml
# .github/workflows/monitoring.yml
name: Monitoring et Alertes

on:
  workflow_run:
    workflows: ["CI/CD Pipeline"]
    types: [completed]

jobs:
  notify-results:
    runs-on: ubuntu-latest

    steps:
      - name: Notification Slack - Succ√®s
        if: ${{ github.event.workflow_run.conclusion == 'success' }}
        uses: 8398a7/action-slack@v3
        with:
          status: success
          channel: "#ci-cd"
          message: |
            ‚úÖ Pipeline CI/CD r√©ussi
            Branche: ${{ github.event.workflow_run.head_branch }}
            Commit: ${{ github.event.workflow_run.head_sha }}
            Auteur: ${{ github.event.workflow_run.head_commit.author.name }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

      - name: Notification Slack - √âchec
        if: ${{ github.event.workflow_run.conclusion == 'failure' }}
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: "#ci-cd-alerts"
          message: |
            ‚ùå Pipeline CI/CD √©chou√©
            Branche: ${{ github.event.workflow_run.head_branch }}
            Commit: ${{ github.event.workflow_run.head_sha }}
            Auteur: ${{ github.event.workflow_run.head_commit.author.name }}

            üîó Voir les d√©tails: ${{ github.event.workflow_run.html_url }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

      - name: Cr√©ation d'issue pour √©chec r√©current
        if: ${{ github.event.workflow_run.conclusion == 'failure' }}
        uses: actions/github-script@v6
        with:
          script: |
            // V√©rifier s'il y a eu des √©checs r√©cents
            const { data: runs } = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: context.payload.workflow_run.workflow_id,
              per_page: 5
            });

            const recentFailures = runs.workflow_runs.filter(run => 
              run.conclusion === 'failure'
            ).length;

            if (recentFailures >= 3) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'üö® √âchecs r√©currents du pipeline CI/CD',
                body: `Le pipeline CI/CD a √©chou√© ${recentFailures} fois r√©cemment.
                
                Derni√®re ex√©cution: ${context.payload.workflow_run.html_url}
                
                **Action requise:** Investigation et correction n√©cessaires.`,
                labels: ['bug', 'ci-cd', 'high-priority']
              });
            }
```

### 2. M√©triques et Tableaux de Bord

```python
# scripts/collect_metrics.py
"""Collecte des m√©triques CI/CD."""

import json
import requests
from datetime import datetime, timedelta


class CIMetricsCollector:
    """Collecteur de m√©triques CI/CD."""

    def __init__(self, github_token, repo):
        self.github_token = github_token
        self.repo = repo
        self.headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }

    def collect_workflow_metrics(self, days=30):
        """Collecte les m√©triques des workflows."""

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        url = f'https://api.github.com/repos/{self.repo}/actions/runs'
        params = {
            'created': f'{start_date.isoformat()}..{end_date.isoformat()}',
            'per_page': 100
        }

        response = requests.get(url, headers=self.headers, params=params)
        runs = response.json()['workflow_runs']

        metrics = {
            'total_runs': len(runs),
            'successful_runs': len([r for r in runs if r['conclusion'] == 'success']),
            'failed_runs': len([r for r in runs if r['conclusion'] == 'failure']),
            'average_duration': self._calculate_average_duration(runs),
            'success_rate': 0,
            'failure_rate': 0,
        }

        if metrics['total_runs'] > 0:
            metrics['success_rate'] = metrics['successful_runs'] / metrics['total_runs'] * 100
            metrics['failure_rate'] = metrics['failed_runs'] / metrics['total_runs'] * 100

        return metrics

    def _calculate_average_duration(self, runs):
        """Calcule la dur√©e moyenne des workflows."""

        durations = []

        for run in runs:
            if run['created_at'] and run['updated_at']:
                created = datetime.fromisoformat(run['created_at'].replace('Z', '+00:00'))
                updated = datetime.fromisoformat(run['updated_at'].replace('Z', '+00:00'))
                duration = (updated - created).total_seconds()
                durations.append(duration)

        return sum(durations) / len(durations) if durations else 0

    def generate_report(self):
        """G√©n√®re un rapport de m√©triques."""

        metrics = self.collect_workflow_metrics()

        report = f"""
# üìä Rapport de M√©triques CI/CD

## P√©riode: 30 derniers jours

### Statistiques G√©n√©rales
- **Total d'ex√©cutions**: {metrics['total_runs']}
- **Succ√®s**: {metrics['successful_runs']} ({metrics['success_rate']:.1f}%)
- **√âchecs**: {metrics['failed_runs']} ({metrics['failure_rate']:.1f}%)
- **Dur√©e moyenne**: {metrics['average_duration']/60:.1f} minutes

### Objectifs de Performance
- ‚úÖ Taux de succ√®s > 95%: {'‚úÖ' if metrics['success_rate'] > 95 else '‚ùå'}
- ‚úÖ Dur√©e moyenne < 10 min: {'‚úÖ' if metrics['average_duration'] < 600 else '‚ùå'}

### Recommandations
"""

        if metrics['success_rate'] < 95:
            report += "- üîß Am√©liorer la stabilit√© des tests\n"

        if metrics['average_duration'] > 600:
            report += "- ‚ö° Optimiser la dur√©e des workflows\n"

        return report


# Utilisation
if __name__ == '__main__':
    import os

    collector = CIMetricsCollector(
        github_token=os.environ['GITHUB_TOKEN'],
        repo='owner/django-graphql-auto'
    )

    report = collector.generate_report()
    print(report)

    # Sauvegarder le rapport
    with open('ci_metrics_report.md', 'w') as f:
        f.write(report)
```

Ce guide d'int√©gration CI/CD fournit une base solide pour automatiser les tests, le d√©ploiement et le monitoring du projet Django GraphQL Auto, garantissant ainsi la qualit√© et la fiabilit√© du code en production.
